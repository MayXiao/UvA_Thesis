{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4803565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:VERSION 2.1.2\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import logging\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid', font_scale=1.4)\n",
    "from glob import glob\n",
    "import wandb\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToTensor, RandomCrop\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import statistics\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import umap\n",
    "from cycler import cycler\n",
    "\n",
    "import pytorch_metric_learning\n",
    "import pytorch_metric_learning.utils.logging_presets as logging_presets\n",
    "from pytorch_metric_learning import losses, miners, samplers, testers, trainers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "from pytorch_metric_learning.distances import CosineSimilarity\n",
    "from pytorch_metric_learning.utils.inference import InferenceModel, MatchFinder\n",
    "from pytorch_metric_learning.utils import common_functions as c_f\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.info(\"VERSION %s\" % pytorch_metric_learning.__version__)\n",
    "\n",
    "import fuzzymatcher\n",
    "from fuzzymatcher import link_table, fuzzy_left_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df2062b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/var/scratch/mxiao/data/'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "19f5743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the image transforms\n",
    "normalize = transforms.Normalize(mean=[0.6195012,0.6195012,0.6195012], std=[0.3307451,0.3307451,0.3307451])\n",
    "# normalize = transforms.Normalize(mean=[0.53997546,0.53997546,0.53997546], std=[0.36844322,0.36844322,0.36844322])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomRotation(10),      # rotate +/- 10 degrees\n",
    "        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "#         transforms.Resize(224),             # resize shortest side to 224 pixels\n",
    "#         transforms.CenterCrop(224),         # crop longest side to 224 pixels at center\n",
    "        transforms.RandomCrop(size=(224,224),pad_if_needed=True), \n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.RandomCrop((224,224),pad_if_needed=True),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "101955c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "dataset1 = datasets.ImageFolder(root=(\"model/train\"),transform=train_transform)\n",
    "dataset2 = datasets.ImageFolder(root=(\"model/test\"),transform=test_transform)\n",
    "\n",
    "class_dict = {i: class_name for i, class_name in enumerate(dataset1.classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3a6106ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset1, batch_size=32, shuffle=True,num_workers=4)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False,num_workers=4)\n",
    "test_loader = DataLoader(dataset2, batch_size=32, shuffle=False,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45f45675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP(nn.Module):\n",
    "#     # layer_sizes[0] is the dimension of the input\n",
    "#     # layer_sizes[-1] is the dimension of the output\n",
    "#     def __init__(self, layer_sizes, final_relu=False):\n",
    "#         super().__init__()\n",
    "#         layer_list = []\n",
    "#         layer_sizes = [int(x) for x in layer_sizes]\n",
    "#         num_layers = len(layer_sizes) - 1\n",
    "#         final_relu_layer = num_layers if final_relu else num_layers - 1\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             input_size = layer_sizes[i]\n",
    "#             curr_size = layer_sizes[i + 1]\n",
    "#             if i < final_relu_layer:\n",
    "#                 layer_list.append(nn.ReLU(inplace=False))\n",
    "#             layer_list.append(nn.Linear(input_size, curr_size))\n",
    "#         self.net = nn.Sequential(*layer_list)\n",
    "#         self.last_linear = self.net[-1]\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.net(x)\n",
    "class MLP(nn.Module):\n",
    "    # layer_sizes[0] is the dimension of the input\n",
    "    # layer_sizes[-1] is the dimension of the output\n",
    "    def __init__(self, layer_sizes, final_relu=False, dropout_rate=0.5): # you can adjust dropout_rate as per your needs\n",
    "        super().__init__()\n",
    "        layer_list = []\n",
    "        layer_sizes = [int(x) for x in layer_sizes]\n",
    "        num_layers = len(layer_sizes) - 1\n",
    "        final_relu_layer = num_layers if final_relu else num_layers - 1\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            input_size = layer_sizes[i]\n",
    "            curr_size = layer_sizes[i + 1]\n",
    "            if i < final_relu_layer:\n",
    "                layer_list.append(nn.ReLU(inplace=False))\n",
    "                layer_list.append(nn.Dropout(dropout_rate))  # add dropout layer after ReLU\n",
    "            layer_list.append(nn.Linear(input_size, curr_size))\n",
    "        self.net = nn.Sequential(*layer_list)\n",
    "        self.last_linear = self.net[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9df81e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d99cb886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resnet\n",
    "# Set trunk model and replace the softmax layer with an identity function\n",
    "trunk = torchvision.models.resnet50(pretrained=True).to(device)\n",
    "trunk_output_size = trunk.fc.in_features\n",
    "trunk.fc = nn.Identity()  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "00eb3f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "best_trunk_weights = glob.glob('PML_v13/trunk_best*.pth'.format('Resnet50'))[0]\n",
    "trunk.load_state_dict(torch.load(best_trunk_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7cec5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedder = MLP([trunk_output_size, 748]).to(device)\n",
    "embedder = MLP([trunk_output_size, 748], dropout_rate=0.5).to(device)  # example with custom dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d46fab27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_embedder_weights = glob.glob('PML_v13/embedder_best*.pth'.format('Resnet50'))[0]\n",
    "embedder.load_state_dict(torch.load(best_embedder_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "58a13887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = nn.DataParallel(MLP([748, len(class_dict)])).to(device)\n",
    "classifier = nn.DataParallel(MLP([748, len(class_dict)], dropout_rate=0.3)).to(device)  # example with custom dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bc857570",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = dict(\n",
    "        epochs = 40\n",
    "#         model = 'resnet101'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "34726773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(classifier.parameters(), lr=0.001, weight_decay=0.1)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3a617a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_loader, model, criterion, optimizer, scheduler):\n",
    "    # Train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Track metrics\n",
    "    loss_epoch = 0\n",
    "    accuracy_epoch = 0\n",
    "    \n",
    "    # Loop over minibatches\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        # Send to device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            backbone_out = trunk(inputs)\n",
    "            embedding_out = embedder(backbone_out)\n",
    "            \n",
    "        outputs = model(embedding_out)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "#         loss.requires_grad = True\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Track loss\n",
    "        loss_epoch += loss.detach().item()\n",
    "        \n",
    "        # Accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        accuracy_epoch += torch.sum(preds == labels)/inputs.shape[0]\n",
    "        \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "        \n",
    "    return loss_epoch/len(train_loader), accuracy_epoch.item()/len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "16bd90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_one_epoch(test_loader, model, criterion):\n",
    "    # Eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Track metrics\n",
    "    loss_epoch = 0\n",
    "    accuracy_epoch = 0\n",
    "    \n",
    "    # Don't update weights\n",
    "    with torch.no_grad():\n",
    "        # Loop over minibatches\n",
    "        for inputs, labels in tqdm(test_loader):\n",
    "            # Send to device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            backbone_out = trunk(inputs)\n",
    "            embedding_out = embedder(backbone_out)\n",
    "            \n",
    "            outputs = model(embedding_out)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Track loss\n",
    "            loss_epoch += loss.detach().item()\n",
    "            \n",
    "            # Accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            accuracy_epoch += torch.sum(preds == labels)/inputs.shape[0]\n",
    "    \n",
    "    return loss_epoch/len(test_loader), accuracy_epoch.item()/len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ab4886cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history\n",
    "def plot_hist(train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist):    \n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_loss_hist, label='Train_Loss')\n",
    "    plt.plot(test_loss_hist, label='Test_loss')\n",
    "    plt.title('Cross Entropy Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_acc_hist, label='Train_Accuracy')\n",
    "    plt.plot(test_acc_hist, label='Val_Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "90728ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, train_loader, test_loader, verbose=True):\n",
    "    # Initialise outputs\n",
    "    train_loss_hist = []\n",
    "    test_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    test_acc_hist = []\n",
    "    best_acc = 0.0\n",
    "    model_path = './emb_res50_1024_model_v1.pth'\n",
    "    \n",
    "    # Loop over epochs\n",
    "    for epoch in range(CFG['epochs']):\n",
    "        # Train\n",
    "        train_loss, train_accuracy = train_one_epoch(train_loader, model, criterion, optimizer, scheduler)\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_accuracy = evaluate_one_epoch(test_loader, model, criterion)\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss_hist.append(train_loss)\n",
    "        test_loss_hist.append(test_loss)\n",
    "        train_acc_hist.append(train_accuracy)\n",
    "        test_acc_hist.append(test_accuracy)\n",
    "        \n",
    "        # Log metrics\n",
    "        wandb.log({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy\n",
    "        })\n",
    "        \n",
    "        if test_accuracy > best_acc:\n",
    "            best_acc = test_accuracy\n",
    "            torch.save(model.state_dict(), model_path) \n",
    "            print('saving model with acc {:.3f}'.format(best_acc))\n",
    "            \n",
    "        # Print loss\n",
    "        if verbose:\n",
    "            if (epoch+1)%1==0:\n",
    "                print(f'Epoch {epoch+1}/{CFG[\"epochs\"]}, loss {train_loss:.5f}, test_loss {test_loss:.5f}, accuracy {train_accuracy:.5f}, test_accuracy {test_accuracy:.5f}')\n",
    "    \n",
    "    return train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "60e7dbc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:07m33omm) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c63a92320c49c58c82e8c31e3c01a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>test_accuracy</td><td>▁▃▄▅▅▆▄▅▇▇▇█▇██▇▆▇█</td></tr><tr><td>test_loss</td><td>▁▁▁▂▃▃▄▄▅▅▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▄▅▆▆▇▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>18</td></tr><tr><td>test_accuracy</td><td>0.02725</td></tr><tr><td>test_loss</td><td>7.25258</td></tr><tr><td>train_accuracy</td><td>0.5663</td></tr><tr><td>train_loss</td><td>1.92241</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deft-mountain-30</strong> at: <a href='https://wandb.ai/xiaomeng9061/comic-classification/runs/07m33omm' target=\"_blank\">https://wandb.ai/xiaomeng9061/comic-classification/runs/07m33omm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230628_173031-07m33omm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:07m33omm). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c3181658614b04acae373d78534b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669392585754395, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/var/scratch/mxiao/data/wandb/run-20230628_225631-641vs0o8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xiaomeng9061/comic-classification/runs/641vs0o8' target=\"_blank\">chocolate-thunder-31</a></strong> to <a href='https://wandb.ai/xiaomeng9061/comic-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xiaomeng9061/comic-classification' target=\"_blank\">https://wandb.ai/xiaomeng9061/comic-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xiaomeng9061/comic-classification/runs/641vs0o8' target=\"_blank\">https://wandb.ai/xiaomeng9061/comic-classification/runs/641vs0o8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialise run\n",
    "run = wandb.init(\n",
    "                 project = 'comic-classification',\n",
    "                 config = CFG,\n",
    "                 save_code = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "277c80e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1114/1114 [01:57<00:00,  9.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 562/562 [01:02<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model with acc 0.017\n",
      "Epoch 1/40, loss 5.26276, test_loss 6.04003, accuracy 0.12238, test_accuracy 0.01657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1114/1114 [02:06<00:00,  8.84it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 562/562 [01:03<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model with acc 0.019\n",
      "Epoch 2/40, loss 4.05254, test_loss 6.01599, accuracy 0.25511, test_accuracy 0.01902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1114/1114 [02:05<00:00,  8.86it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 562/562 [01:03<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40, loss 3.54829, test_loss 6.08539, accuracy 0.32170, test_accuracy 0.01802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1114/1114 [02:05<00:00,  8.90it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 562/562 [01:03<00:00,  8.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model with acc 0.023\n",
      "Epoch 4/40, loss 3.25126, test_loss 6.16455, accuracy 0.35773, test_accuracy 0.02285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1114/1114 [02:04<00:00,  8.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 562/562 [01:02<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40, loss 3.04045, test_loss 6.25099, accuracy 0.38544, test_accuracy 0.02274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1114/1114 [02:04<00:00,  8.91it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 562/562 [01:02<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model with acc 0.023\n",
      "Epoch 6/40, loss 2.90461, test_loss 6.34745, accuracy 0.40239, test_accuracy 0.02313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1114/1114 [02:04<00:00,  8.93it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 562/562 [01:02<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model with acc 0.023\n",
      "Epoch 7/40, loss 2.80178, test_loss 6.42081, accuracy 0.41533, test_accuracy 0.02330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1114/1114 [02:04<00:00,  8.93it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 562/562 [01:02<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model with acc 0.024\n",
      "Epoch 8/40, loss 2.70563, test_loss 6.51785, accuracy 0.42555, test_accuracy 0.02363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1114/1114 [02:04<00:00,  8.95it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 562/562 [01:02<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model with acc 0.025\n",
      "Epoch 9/40, loss 2.64207, test_loss 6.59020, accuracy 0.43578, test_accuracy 0.02452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 1114/1114 [02:04<00:00,  8.93it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 562/562 [01:02<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model with acc 0.026\n",
      "Epoch 10/40, loss 2.58473, test_loss 6.65070, accuracy 0.44066, test_accuracy 0.02558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████▉                                                        | 333/1114 [00:37<01:28,  8.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[117], line 13\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, train_loader, test_loader, verbose)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Loop over epochs\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(CFG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m evaluate_one_epoch(test_loader, model, criterion)\n",
      "Cell \u001b[0;32mIn[114], line 34\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(train_loader, model, criterion, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Track loss\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Accuracy\u001b[39;00m\n\u001b[1;32m     37\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_loss_hist, test_loss_hist, train_acc_hist, test_acc_hist = train_model(classifier, criterion, optimizer, scheduler, train_loader, test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef4f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
